# ========================================
# 第一步：导入必要的库（按功能分组）
# ========================================
# 1.1 基础库导入
import os  # 操作系统接口
import copy  # 深拷贝功能
import numpy as np  # 数值计算库

# 1.2 PyTorch深度学习框架相关库（并列导入）
import torch  # PyTorch核心库
import torch.nn as nn  # 神经网络模块
import torch.optim as optim  # 优化器模块
from torch.utils.data import Dataset, DataLoader, random_split  # 数据处理工具

# 1.3 计算机视觉相关库（并列导入）
import torchvision.transforms as transforms  # 图像变换工具
import torchvision.models as models  # 预训练模型库
import cv2  # OpenCV计算机视觉库

# 1.4 音频处理相关库（并列导入）
import librosa  # 音频处理库
from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor  # Wav2Vec2音频模型

# ========================================
# 第二步：全局配置设置（按优先级顺序）
# ========================================
# 2.1 首先配置计算设备 - 检查是否有GPU可用
设备 = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 2.2 然后定义视频帧的图像变换管道（按处理顺序）
图像变换 = transforms.Compose([
    transforms.Resize((224, 224)),  # 步骤1：调整图像大小为224x224
    transforms.ToTensor(),  # 步骤2：转换为张量格式
    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # 步骤3：ImageNet标准化均值
                         std=[0.229, 0.224, 0.225])  # 步骤3：ImageNet标准化标准差
])

# 2.3 最后加载Wav2Vec2特征提取器（用于音频预处理）
特征提取器 = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-base-960h')

# ========================================
# 第三步：定义多模态抑郁症数据集类
# ========================================
class 抑郁症数据集(Dataset):
    def __init__(self, 数据列表, 帧数=16):
        """
        数据列表: 包含元组(视频路径, 音频路径, 抑郁症评分)的列表
        帧数: 从每个视频中采样的帧数
        """
        # 3.1 初始化数据集参数（并列初始化）
        self.数据列表 = 数据列表
        self.帧数 = 帧数

    def __len__(self):
        # 3.2 返回数据集大小
        return len(self.数据列表)

    def __getitem__(self, 索引):
        # 3.3 数据获取流程（按步骤执行）
        # 3.3.1 解析数据项
        视频路径, 音频路径, 抑郁症评分 = self.数据列表[索引]
        
        # 3.3.2 视频处理流程（顺序执行）
        # 3.3.2.1 初始化视频处理
        帧列表 = []
        try:
            # 3.3.2.1.1 打开视频文件
            视频捕获 = cv2.VideoCapture(视频路径)
        except Exception as e:
            # 3.3.2.1.2 错误处理：如果打开视频文件失败，抛出运行时错误
            raise RuntimeError(f"打开视频文件 {视频路径} 时出错: {e}")
        
        # 3.3.2.2 获取视频基本信息
        总帧数 = int(视频捕获.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # 3.3.2.3 制定帧采样策略（条件分支）
        if 总帧数 <= 0:
            # 3.3.2.3.1 分支1：如果无法获取帧数，则读取到结束
            帧索引列表 = None
        else:
            # 3.3.2.3.2 分支2：确定要均匀采样的帧索引
            采样数量 = min(self.帧数, 总帧数)
            # 3.3.2.3.2.1 根据视频长度调整采样策略
            if 采样数量 < self.帧数:
                步长 = 1  # 短视频：每帧都采样
            else:
                步长 = 总帧数 // 采样数量  # 长视频：均匀间隔采样
            帧索引列表 = [i * 步长 for i in range(采样数量)]
        
        # 3.3.2.4 初始化帧读取计数器（并列初始化）
        当前帧索引 = 0
        已采样数量 = 0
        
        # 3.3.2.5 执行帧采样循环（主要处理流程）
        while True:
            # 3.3.2.5.1 读取下一帧
            成功读取, 帧 = 视频捕获.read()
            if not 成功读取:
                break  # 3.3.2.5.1.1 读取完毕，退出循环
            
            # 3.3.2.5.2 判断是否需要采样当前帧
            if 帧索引列表 is None or 当前帧索引 in 帧索引列表:
                # 3.3.2.5.3 帧处理流水线（按顺序执行）
                帧_RGB = cv2.cvtColor(帧, cv2.COLOR_BGR2RGB)  # 3.3.2.5.3.1 颜色空间转换
                帧_PIL = transforms.functional.to_pil_image(帧_RGB)  # 3.3.2.5.3.2 转换为PIL格式
                帧_张量 = 图像变换(帧_PIL)  # 3.3.2.5.3.3 应用图像变换
                帧列表.append(帧_张量)  # 3.3.2.5.3.4 添加到帧列表
                已采样数量 += 1
                
                if 已采样数量 >= self.帧数:
                    # 3.3.2.5.4 采样完成检查：已收集所需帧数
                    break
            当前帧索引 += 1  # 3.3.2.5.5 更新帧索引计数器
        
        # 3.3.2.6 清理视频资源
        视频捕获.release()
        
        # 3.3.2.7 帧数据后处理（条件分支处理）
        if len(帧列表) < self.帧数:
            # 3.3.2.7.1 分支1：帧数不足的处理
            if len(帧列表) > 0:
                # 3.3.2.7.1.1 有部分帧：重复最后一帧进行填充
                最后一帧 = 帧列表[-1]
                while len(帧列表) < self.帧数:
                    帧列表.append(最后一帧)
            else:
                # 3.3.2.7.1.2 无有效帧：创建零填充的虚拟帧
                帧列表 = [torch.zeros(3, 224, 224) for _ in range(self.帧数)]
        
        # 3.3.2.8 视频数据最终整合
        帧张量 = torch.stack(帧列表)  # 将帧堆叠成形状为(帧数, 3, 224, 224)的张量

        # 3.3.3 音频处理流程（顺序执行）
        # 3.3.3.1 音频文件加载和预处理
        音频数据, 采样率 = librosa.load(音频路径, sr=16000)  # 加载并重采样到16kHz（单声道）
        # 3.3.3.2 音频数据类型转换
        音频数据 = 音频数据.astype(np.float32)  # librosa默认混合为单声道并重采样

        # 3.3.4 标签生成流程（条件分支）
        # 3.3.4.1 根据评分定义抑郁症等级标签（使用4级分类阈值）
        # 3.3.4.1.1 无抑郁：0-4分
        if 抑郁症评分 < 5:
            类别标签 = 0
        # 3.3.4.1.2 轻度抑郁：5-9分
        elif 抑郁症评分 < 10:
            类别标签 = 1
        # 3.3.4.1.3 中度抑郁：10-14分
        elif 抑郁症评分 < 15:
            类别标签 = 2
        # 3.3.4.1.4 重度抑郁：>=15分
        else:
            类别标签 = 3

        # 3.3.5 返回处理结果（并列返回四个数据项）
        return 帧张量, 音频数据, float(抑郁症评分), 类别标签

# ========================================
# 第四步：定义批处理函数（用于DataLoader处理变长音频）
# ========================================
def 批处理函数(批次):
    # 4.1 批处理流程（按步骤执行）
    # 4.1.1 初始化批次容器（并列初始化四个列表）
    帧批次 = []
    音频批次 = []
    评分批次 = []
    标签批次 = []
    
    # 4.1.2 收集批次数据（循环处理每个样本）
    for 帧张量, 音频数据, 评分, 标签 in 批次:
        # 4.1.2.1 并列添加各类数据到对应批次列表
        帧批次.append(帧张量)
        音频批次.append(音频数据)
        评分批次.append(评分)
        标签批次.append(标签)
    
    # 4.1.3 批次数据整合（按数据类型分别处理）
    # 4.1.3.1 视频数据整合：堆叠帧张量
    帧批次 = torch.stack(帧批次)  # 形状：(批次大小, 帧数, 3, 224, 224)
    
    # 4.1.3.2 音频数据整合：使用特征提取器处理变长音频
    编码输入 = 特征提取器(音频批次, sampling_rate=16000, return_tensors="pt", padding=True)
    音频输入 = 编码输入.input_values  # 4.1.3.2.1 获取填充后的音频数据，形状：(批次大小, 最大长度)
    音频注意力掩码 = 编码输入.attention_mask  # 4.1.3.2.2 获取注意力掩码，形状：(批次大小, 最大长度)
    
    # 4.1.3.3 标签数据整合：转换为张量（并列处理两种标签）
    评分批次 = torch.tensor(评分批次, dtype=torch.float32)  # 回归标签
    标签批次 = torch.tensor(标签批次, dtype=torch.long)     # 分类标签
    
    # 4.1.4 返回整合后的批次数据（按固定顺序返回五个张量）
    return 帧批次, 音频输入, 音频注意力掩码, 评分批次, 标签批次

# ========================================
# 第五步：数据准备和分割（按顺序执行数据集构建流程）
# ========================================
# 准备数据（示例用法；根据需要替换为实际数据加载）
# 这里数据列表应该是(视频路径, 音频路径, 评分)元组的列表
# 例如，使用CSV或预定义的文件路径和评分列表：
# 视频路径列表 = ["data/video1.mp4", "data/video2.mp4", ...]
# 音频路径列表 = ["data/audio1.wav", "data/audio2.wav", ...]
# 评分列表 = [评分1, 评分2, ...]
# 数据列表 = list(zip(视频路径列表, 音频路径列表, 评分列表))
数据列表 = []  # TODO: 用实际的(视频路径, 音频路径, 评分)数据填充

# 如果使用CSV文件：
# import pandas as pd
# df = pd.read_csv('data_info.csv')
# 数据列表 = list(zip(df['video_path'], df['audio_path'], df['depression_score']))

# 5.1 数据有效性检查
# 5.1.1 检查数据列表是否为空
if len(数据列表) == 0:
    # 5.1.2 如果数据列表为空，抛出运行时错误
    raise RuntimeError("数据列表为空。请用实际数据集信息填充数据列表。")

# 5.2 创建数据集实例
# 5.2.1 使用数据列表初始化抑郁症数据集对象
数据集 = 抑郁症数据集(数据列表)

# 5.3 计算数据集分割比例（并列计算三个数据集大小）
# 5.3.1 获取数据集总样本数
总样本数 = len(数据集)
# 5.3.2 计算训练集大小（占总数据的80%）
训练集大小 = int(0.8 * 总样本数)
# 5.3.3 计算验证集大小（占总数据的10%）
验证集大小 = int(0.1 * 总样本数)
# 5.3.4 计算测试集大小（占剩余的数据）
测试集大小 = 总样本数 - 训练集大小 - 验证集大小

# 5.4 执行数据集随机分割
# 5.4.1 使用random_split函数将数据集随机分割为训练、验证和测试三个子集
训练数据集, 验证数据集, 测试数据集 = random_split(数据集, [训练集大小, 验证集大小, 测试集大小])

# 5.5 配置批次大小
# 5.5.1 设置批次大小为4（适合小数据集的批次大小）
批次大小 = 4

# 5.6 创建数据加载器（并列创建三个数据加载器）
# 5.6.1 创建训练数据加载器（启用数据打乱以增加随机性）
训练加载器 = DataLoader(训练数据集, batch_size=批次大小, shuffle=True, collate_fn=批处理函数)
# 5.6.2 创建验证数据加载器（不打乱数据以保持一致性）
验证加载器 = DataLoader(验证数据集, batch_size=批次大小, shuffle=False, collate_fn=批处理函数)
# 5.6.3 创建测试数据加载器（不打乱数据以保持一致性）
测试加载器 = DataLoader(测试数据集, batch_size=批次大小, shuffle=False, collate_fn=批处理函数)

# ========================================
# 第六步：定义多模态模型（ResNet + Wav2Vec2 + LSTM + 融合 + 回归和分类）
# ========================================
class 多模态抑郁症模型(nn.Module):
    def __init__(self, 类别数=4):
        super(多模态抑郁症模型, self).__init__()
        
        # 6.1 模型初始化流程（按组件类型分步构建）
        # 6.1.1 视频处理组件初始化（顺序构建视频特征提取器）
        # 6.1.1.1 加载预训练的ResNet18作为视觉CNN用于面部帧特征提取
        self.resnet = models.resnet18(pretrained=True)
        # 6.1.1.2 将最终全连接层替换为恒等映射以获得特征向量
        self.resnet.fc = nn.Identity()
        # 6.1.1.3 冻结ResNet参数（我们将其用作固定特征提取器）
        for param in self.resnet.parameters():
            param.requires_grad = False
        # 6.1.1.4 将ResNet设置为评估模式（冻结批标准化等）
        self.resnet.eval()

        # 6.1.2 音频处理组件初始化（顺序构建音频特征提取器）
        # 6.1.2.1 加载预训练的Wav2Vec2模型作为音频特征提取器
        self.wav2vec = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-base-960h')
        # 6.1.2.2 同样冻结Wav2Vec2参数
        for param in self.wav2vec.parameters():
            param.requires_grad = False
        # 6.1.2.3 将Wav2Vec2设置为评估模式
        self.wav2vec.eval()

        # 6.1.3 序列处理组件初始化
        # 6.1.3.1 构建LSTM处理来自Wav2Vec2的音频特征序列
        self.音频LSTM = nn.LSTM(input_size=768, hidden_size=128, num_layers=1,
                                  batch_first=True, dropout=0.0, bidirectional=False)
        
        # 6.1.4 特征融合和输出组件初始化（按处理顺序构建）
        # 6.1.4.1 初始化dropout层用于正则化
        self.dropout = nn.Dropout(p=0.5)
        # 6.1.4.2 构建融合全连接层（融合视频和音频特征）
        self.融合全连接层 = nn.Linear(512 + 128, 128)    # 来自ResNet的512维 + 来自LSTM的128维
        # 6.1.4.3 构建回归输出层（预测连续的抑郁症评分）
        self.回归全连接层 = nn.Linear(128, 1)        # 回归输出（抑郁症评分）
        # 6.1.4.4 构建分类输出层（预测离散的抑郁症等级）
        self.分类全连接层 = nn.Linear(128, 类别数)  # 分类输出（抑郁症等级）

    def forward(self, 帧, 音频输入, 音频掩码):
        # 6.2 前向传播流程（按数据流顺序执行）
        # 输入参数说明：
        # 帧: 形状为(批次大小, 帧数, 3, 224, 224)的张量
        # 音频输入: 形状为(批次大小, 序列长度)的音频波形张量
        # 音频掩码: 音频输入的注意力掩码(批次大小, 序列长度)
        
        # 6.2.1 视频数据预处理（准备视频帧数据）
        批次大小, 帧数, C, H, W = 帧.size()
        # 6.2.1.1 合并批次和帧维度以一次性通过ResNet
        帧扁平化 = 帧.view(批次大小 * 帧数, C, H, W)
        
        # 6.2.2 视频特征提取（使用冻结的ResNet）
        # 6.2.2.1 使用ResNet提取视觉特征（无梯度计算）
        with torch.no_grad():
            视觉特征扁平化 = self.resnet(帧扁平化)  # 形状: (批次大小*帧数, 512)
        
        # 6.2.3 视频特征后处理（时序聚合）
        # 6.2.3.1 重塑回(批次大小, 帧数, 特征维度)格式
        视觉特征序列 = 视觉特征扁平化.view(批次大小, 帧数, -1)
        # 6.2.3.2 在帧维度上进行平均池化，得到视频级特征
        视频特征 = 视觉特征序列.mean(dim=1)  # 形状: (批次大小, 512)

        # 6.2.4 音频特征提取（使用冻结的Wav2Vec2）
        # 6.2.4.1 使用Wav2Vec2提取音频特征（无梯度计算）
        with torch.no_grad():
            音频输出 = self.wav2vec(音频输入, attention_mask=音频掩码)
        # 6.2.4.2 获取最后一层隐藏状态作为音频特征序列
        音频特征序列 = 音频输出.last_hidden_state  # 形状: (批次大小, 序列长度_特征, 768)
        
        # 6.2.5 音频序列处理（使用LSTM）
        # 6.2.5.1 使用LSTM处理音频特征序列
        LSTM输出, (h_n, c_n) = self.音频LSTM(音频特征序列)  # h_n形状: (1, 批次大小, 128)
        # 6.2.5.2 提取最后LSTM隐藏状态作为音频特征
        音频特征 = h_n[-1]  # 每个批次元素的最后LSTM隐藏状态，形状: (批次大小, 128)

        # 6.2.6 多模态特征融合（按顺序执行融合操作）
        # 6.2.6.1 连接视觉和音频特征
        融合特征 = torch.cat([视频特征, 音频特征], dim=1)  # 形状: (批次大小, 640)
        
        # 6.2.7 特征变换和正则化（按顺序应用变换层）
        # 6.2.7.1 第一次dropout正则化
        x = self.dropout(融合特征)
        # 6.2.7.2 通过融合全连接层并应用ReLU激活
        x = torch.relu(self.融合全连接层(x))
        # 6.2.7.3 第二次dropout正则化
        x = self.dropout(x)
        
        # 6.2.8 生成最终输出（并列生成两个任务的输出）
        # 6.2.8.1 回归任务：预测连续的抑郁症评分
        评分预测 = self.回归全连接层(x).squeeze(1)     # 回归输出，形状: (批次大小,)
        # 6.2.8.2 分类任务：预测离散的抑郁症等级
        类别logits = self.分类全连接层(x)          # 分类输出（logits），形状: (批次大小, 类别数)
        
        # 6.2.9 返回处理结果（按固定顺序返回两个输出）
        return 评分预测, 类别logits

# ========================================
# 第七步：模型初始化和训练配置（按组件类型分步初始化）
# ========================================
# 7.1 模型实例化
模型 = 多模态抑郁症模型(类别数=4).to(设备)  # 创建模型并移动到指定设备

# 7.2 损失函数配置（并列配置两个任务的损失函数）
MSE损失函数 = nn.MSELoss()        # 7.2.1 回归任务：均方误差损失
交叉熵损失函数 = nn.CrossEntropyLoss()  # 7.2.2 分类任务：交叉熵损失

# 7.3 优化器配置
# 7.3.1 配置Adam优化器（包含学习率和L2正则化）
优化器 = optim.Adam(模型.parameters(), lr=1e-3, weight_decay=1e-5)

# ========================================
# 第八步：训练循环（早停机制）
# ========================================
# 8.1 训练参数初始化（并列初始化训练控制参数）
# 8.1.1 设置最大训练轮数
训练轮数 = 50
# 8.1.2 设置早停耐心值（连续多少轮无改善后停止训练）
耐心值 = 5
# 8.1.3 初始化最佳验证损失为无穷大
最佳验证损失 = float('inf')
# 8.1.4 保存初始模型权重作为最佳模型权重
最佳模型权重 = copy.deepcopy(模型.state_dict())
# 8.1.5 初始化无改善轮数计数器
无改善轮数 = 0

# 8.2 主训练循环（按轮次顺序执行）
for 轮次 in range(1, 训练轮数 + 1):
    # 8.2.1 训练阶段（模型参数更新）
    # 8.2.1.1 设置模型为训练模式（启用dropout和batch normalization的训练行为）
    模型.train()
    # 8.2.1.2 保持预训练特征提取器为评估模式（冻结状态，不更新参数）
    # 8.2.1.2.1 保持ResNet为评估模式（冻结视觉特征提取器）
    模型.resnet.eval()
    # 8.2.1.2.2 保持Wav2Vec2为评估模式（冻结音频特征提取器）
    模型.wav2vec.eval()
    
    # 8.2.1.3 初始化训练损失累计器
    运行损失 = 0.0
    
    # 8.2.1.4 训练批次循环（逐批次处理训练数据）
    for 帧, 音频输入, 音频掩码, 评分, 标签 in 训练加载器:
        # 8.2.1.4.1 数据移动到设备（并列移动五个张量到GPU/CPU）
        # 8.2.1.4.1.1 移动视频帧数据到计算设备
        帧 = 帧.to(设备)
        # 8.2.1.4.1.2 移动音频输入数据到计算设备
        音频输入 = 音频输入.to(设备)
        # 8.2.1.4.1.3 移动音频注意力掩码到计算设备
        音频掩码 = 音频掩码.to(设备)
        # 8.2.1.4.1.4 移动回归标签（评分）到计算设备
        评分 = 评分.to(设备)
        # 8.2.1.4.1.5 移动分类标签到计算设备
        标签 = 标签.to(设备)
        
        # 8.2.1.4.2 前向传播（获取模型预测）
        # 8.2.1.4.2.1 通过模型获取预测结果（回归评分和分类logits）
        预测评分, 类别logits = 模型(帧, 音频输入, 音频掩码)
        
        # 8.2.1.4.3 损失计算（并列计算两个任务的损失）
        # 8.2.1.4.3.1 计算回归任务损失（预测评分与真实评分的均方误差）
        回归损失 = MSE损失函数(预测评分, 评分)
        # 8.2.1.4.3.2 计算分类任务损失（预测类别与真实类别的交叉熵）
        分类损失 = 交叉熵损失函数(类别logits, 标签)
        # 8.2.1.4.3.3 计算总损失（两个任务损失的简单相加）
        总损失 = 回归损失 + 分类损失
        
        # 8.2.1.4.4 反向传播和参数更新（按顺序执行优化步骤）
        # 8.2.1.4.4.1 清零之前累积的梯度
        优化器.zero_grad()
        # 8.2.1.4.4.2 计算梯度（反向传播）
        总损失.backward()
        # 8.2.1.4.4.3 更新模型参数
        优化器.step()
        
        # 8.2.1.4.5 累计训练损失
        运行损失 += 总损失.item() * 帧.size(0)
    
    # 8.2.1.5 计算当前轮次的平均训练损失
    轮次训练损失 = 运行损失 / len(训练加载器.dataset)

    # 8.2.2 验证阶段（模型性能评估）
    # 8.2.2.1 设置模型为评估模式（禁用dropout和batch normalization的训练行为）
    模型.eval()
    
    # 8.2.2.2 初始化验证损失累计器
    验证损失 = 0.0
    
    # 8.2.2.3 验证批次循环（无梯度计算）
    with torch.no_grad():
        # 8.2.2.3.1 逐批次处理验证数据
        for 帧, 音频输入, 音频掩码, 评分, 标签 in 验证加载器:
            # 8.2.2.3.1.1 数据移动到设备（并列移动五个张量到GPU/CPU）
            # 8.2.2.3.1.1.1 移动视频帧数据到计算设备
            帧 = 帧.to(设备)
            # 8.2.2.3.1.1.2 移动音频输入数据到计算设备
            音频输入 = 音频输入.to(设备)
            # 8.2.2.3.1.1.3 移动音频注意力掩码到计算设备
            音频掩码 = 音频掩码.to(设备)
            # 8.2.2.3.1.1.4 移动回归标签（评分）到计算设备
            评分 = 评分.to(设备)
            # 8.2.2.3.1.1.5 移动分类标签到计算设备
            标签 = 标签.to(设备)
            
            # 8.2.2.3.1.2 前向传播（获取模型预测）
            # 8.2.2.3.1.2.1 通过模型获取预测结果（回归评分和分类logits）
            预测评分, 类别logits = 模型(帧, 音频输入, 音频掩码)
            
            # 8.2.2.3.1.3 损失计算（并列计算两个任务的损失）
            # 8.2.2.3.1.3.1 计算回归任务损失（预测评分与真实评分的均方误差）
            回归损失 = MSE损失函数(预测评分, 评分)
            # 8.2.2.3.1.3.2 计算分类任务损失（预测类别与真实类别的交叉熵）
            分类损失 = 交叉熵损失函数(类别logits, 标签)
            # 8.2.2.3.1.3.3 计算总损失（两个任务损失的简单相加）
            总损失 = 回归损失 + 分类损失
            
            # 8.2.2.3.1.4 累计验证损失
            验证损失 += 总损失.item() * 帧.size(0)
    
    # 8.2.2.4 计算当前轮次的平均验证损失
    轮次验证损失 = 验证损失 / len(验证加载器.dataset)
    
    # 8.2.2.5 输出当前轮次的训练和验证损失
    print(f"轮次 {轮次}/{训练轮数} - 训练损失: {轮次训练损失:.4f}, 验证损失: {轮次验证损失:.4f}")
    
    # 8.2.3 早停机制（根据验证损失决定是否继续训练）
    # 8.2.3.1 检查是否有改善
    if 轮次验证损失 < 最佳验证损失:
        # 8.2.3.1.1 有改善：更新最佳验证损失和模型权重
        # 8.2.3.1.1.1 更新最佳验证损失
        最佳验证损失 = 轮次验证损失
        # 8.2.3.1.1.2 保存当前最佳模型权重
        最佳模型权重 = copy.deepcopy(模型.state_dict())
        # 8.2.3.1.1.3 重置无改善轮数计数器
        无改善轮数 = 0
    else:
        # 8.2.3.1.2 无改善：增加无改善轮数计数器
        # 8.2.3.1.2.1 增加无改善轮数
        无改善轮数 += 1
        # 8.2.3.1.2.2 检查是否达到早停条件
        if 无改善轮数 >= 耐心值:
            # 8.2.3.1.2.2.1 触发早停
            print("触发早停。")
            break

# ========================================
# 第九步：模型测试和评估（使用最佳模型权重）
# ========================================
# 9.1 加载最佳模型权重
# 9.1.1 将训练过程中保存的最佳模型权重加载到模型中
模型.load_state_dict(最佳模型权重)

# 9.2 设置模型为评估模式
# 9.2.1 设置模型为评估模式（禁用dropout和batch normalization的训练行为）
模型.eval()

# 9.3 初始化测试结果收集器（并列初始化四个列表）
# 9.3.1 收集回归预测和真实值（转换为CPU并添加到列表）
真实评分 = []
# 9.3.2 收集预测评分
预测评分列表 = []
# 9.3.3 收集分类预测统计（并列初始化两个计数器）
总数 = 0
# 9.3.4 收集正确分类数量
正确数 = 0

# 9.4 测试批次循环（无梯度计算）
with torch.no_grad():
    # 9.4.1 逐批次处理测试数据
    for 帧, 音频输入, 音频掩码, 评分, 标签 in 测试加载器:
        # 9.4.1.1 数据移动到设备（并列移动五个张量到GPU/CPU）
        # 9.4.1.1.1 移动视频帧数据到计算设备
        帧 = 帧.to(设备)
        # 9.4.1.1.2 移动音频输入数据到计算设备
        音频输入 = 音频输入.to(设备)
        # 9.4.1.1.3 移动音频注意力掩码到计算设备
        音频掩码 = 音频掩码.to(设备)
        # 9.4.1.1.4 移动回归标签（评分）到计算设备
        评分 = 评分.to(设备)
        # 9.4.1.1.5 移动分类标签到计算设备
        标签 = 标签.to(设备)
        
        # 9.4.1.2 前向传播（获取模型预测）
        # 9.4.1.2.1 通过模型获取预测结果（回归评分和分类logits）
        预测评分批次, 类别logits = 模型(帧, 音频输入, 音频掩码)
        
        # 9.4.1.3 收集回归预测和真实值（转换为CPU并添加到列表）
        # 9.4.1.3.1 收集真实评分
        真实评分.extend(评分.cpu().numpy().tolist())
        # 9.4.1.3.2 收集预测评分
        预测评分列表.extend(预测评分批次.cpu().numpy().tolist())
        
        # 9.4.1.4 收集分类预测统计
        # 9.4.1.4.1 获取预测类别（取logits的最大值索引）
        预测类别 = torch.argmax(类别logits, dim=1)
        # 9.4.1.4.2 统计正确分类数量
        正确数 += (预测类别 == 标签.to(设备)).sum().item()
        # 9.4.1.4.3 统计总样本数量
        总数 += 标签.size(0)

# ========================================
# 第十步：计算和输出测试指标
# ========================================
# 10.1 数据格式转换（将列表转换为NumPy数组以便计算）
# 10.1.1 转换真实评分为NumPy数组
真实评分数组 = np.array(真实评分)
# 10.1.2 转换预测评分为NumPy数组
预测评分数组 = np.array(预测评分列表)

# 10.2 回归指标计算（并列计算三个回归指标）
# 10.2.1 计算均方误差（MSE）
MSE = np.mean((真实评分数组 - 预测评分数组) ** 2)
# 10.2.2 计算平均绝对误差（MAE）
MAE = np.mean(np.abs(真实评分数组 - 预测评分数组))

# 10.2.3 计算决定系数（R²）
# 10.2.3.1 检查数据有效性
if 真实评分数组.size > 0:
    # 10.2.3.1.1 有效数据：计算R²
    # 10.2.3.1.1.1 计算残差平方和
    残差平方和 = np.sum((真实评分数组 - 预测评分数组) ** 2)
    # 10.2.3.1.1.2 计算总平方和
    总平方和 = np.sum((真实评分数组 - np.mean(真实评分数组)) ** 2)
    # 10.2.3.1.1.3 计算R²值
    R2 = 1 - 残差平方和 / 总平方和 if 总平方和 != 0 else 0.0
else:
    # 10.2.3.1.2 无效数据：设置R²为0
    R2 = 0.0

# 10.3 分类指标计算
# 10.3.1 计算分类准确率
准确率 = 正确数 / 总数 if 总数 > 0 else 0.0

# 10.4 输出测试结果（按指标类型分组输出）
# 10.4.1 输出回归指标
# 10.4.1.1 输出均方误差
print(f"测试MSE: {MSE:.4f}")
# 10.4.1.2 输出平均绝对误差
print(f"测试MAE: {MAE:.4f}")
# 10.4.1.3 输出决定系数
print(f"测试R²: {R2:.4f}")
# 10.4.2 输出分类指标
# 10.4.2.1 输出分类准确率
print(f"测试分类准确率: {准确率*100:.2f}%")